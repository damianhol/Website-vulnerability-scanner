"use strict";Object.defineProperty(exports, "__esModule", {value: true}); function _interopRequireWildcard(obj) { if (obj && obj.__esModule) { return obj; } else { var newObj = {}; if (obj != null) { for (var key in obj) { if (Object.prototype.hasOwnProperty.call(obj, key)) { newObj[key] = obj[key]; } } } newObj.default = obj; return newObj; } } function _nullishCoalesce(lhs, rhsFn) { if (lhs != null) { return lhs; } else { return rhsFn(); } } function _optionalChain(ops) { let lastAccessLHS = undefined; let value = ops[0]; let i = 1; while (i < ops.length) { const op = ops[i]; const fn = ops[i + 1]; i += 2; if ((op === 'optionalAccess' || op === 'optionalCall') && value == null) { return undefined; } if (op === 'access' || op === 'optionalAccess') { lastAccessLHS = value; value = fn(value); } else if (op === 'call' || op === 'optionalCall') { value = fn((...args) => value.call(lastAccessLHS, ...args)); lastAccessLHS = undefined; } } return value; }// src/utils/isBackend.ts
var isBrowser = typeof window !== "undefined" && typeof window.document !== "undefined";
var isWebWorker = typeof self === "object" && self.constructor && self.constructor.name === "DedicatedWorkerGlobalScope";
var isBackend = !isBrowser && !isWebWorker;

// src/utils/isFrontend.ts
var isFrontend = !isBackend;

// src/consts.ts
var HUB_URL = typeof process !== "undefined" && process.env.NODE_ENV === "test" ? "https://hub-ci.huggingface.co" : "https://huggingface.co";

// src/error.ts
async function createApiError(response, opts) {
  const error = new HubApiError(response.url, response.status, _nullishCoalesce(response.headers.get("X-Request-Id"), () => ( _optionalChain([opts, 'optionalAccess', _2 => _2.requestId]))));
  error.message = `Api error with status ${error.statusCode}.${_optionalChain([opts, 'optionalAccess', _3 => _3.message]) ? ` ${opts.message}.` : ""} Request ID: ${error.requestId}, url: ${error.url}`;
  if (_optionalChain([response, 'access', _4 => _4.headers, 'access', _5 => _5.get, 'call', _6 => _6("Content-Type"), 'optionalAccess', _7 => _7.startsWith, 'call', _8 => _8("application/json")])) {
    const json = await response.json();
    error.message = json.error || json.message || error.message;
    error.data = json;
  } else {
    error.data = { message: await response.text() };
  }
  throw error;
}
var HubApiError = class extends Error {
  
  
  
  
  constructor(url, statusCode, requestId, message) {
    super(message);
    this.statusCode = statusCode;
    this.requestId = requestId;
    this.url = url;
  }
};
var InvalidApiResponseFormatError = class extends Error {
};

// src/utils/base64FromBytes.ts
function base64FromBytes(arr) {
  if (globalThis.Buffer) {
    return globalThis.Buffer.from(arr).toString("base64");
  } else {
    const bin = [];
    arr.forEach((byte) => {
      bin.push(String.fromCharCode(byte));
    });
    return globalThis.btoa(bin.join(""));
  }
}

// src/utils/checkCredentials.ts
function checkCredentials(credentials) {
  if (!credentials || credentials.accessToken === void 0 || credentials.accessToken === null) {
    return;
  }
  if (!credentials.accessToken.startsWith("hf_")) {
    throw new TypeError("Your access token must start with 'hf_'");
  }
}

// src/utils/range.ts
function range(n, b) {
  return b ? Array(b - n).fill(0).map((_, i) => n + i) : Array(n).fill(0).map((_, i) => i);
}

// src/utils/chunk.ts
function chunk(arr, chunkSize) {
  if (isNaN(chunkSize) || chunkSize < 1) {
    throw new RangeError("Invalid chunk size: " + chunkSize);
  }
  if (!arr.length) {
    return [];
  }
  if (arr.length <= chunkSize) {
    return [arr];
  }
  return range(Math.ceil(arr.length / chunkSize)).map((i) => {
    return arr.slice(i * chunkSize, (i + 1) * chunkSize);
  });
}

// src/utils/promisesQueue.ts
async function promisesQueue(factories, concurrency) {
  const promises = [];
  const executing = [];
  for (const factory of factories) {
    const p = factory();
    promises.push(p);
    const e = p.then(() => {
      executing.splice(executing.indexOf(e), 1);
    });
    executing.push(e);
    if (executing.length >= concurrency) {
      await Promise.race(executing);
    }
  }
  return Promise.all(promises);
}

// src/utils/promisesQueueStreaming.ts
async function promisesQueueStreaming(factories, concurrency) {
  const executing = [];
  for await (const factory of factories) {
    const e = factory().then(() => {
      executing.splice(executing.indexOf(e), 1);
    });
    executing.push(e);
    if (executing.length >= concurrency) {
      await Promise.race(executing);
    }
  }
  await Promise.all(executing);
}

// src/utils/hexFromBytes.ts
function hexFromBytes(arr) {
  if (globalThis.Buffer) {
    return globalThis.Buffer.from(arr).toString("hex");
  } else {
    const bin = [];
    arr.forEach((byte) => {
      bin.push(byte.toString(16).padStart(2, "0"));
    });
    return bin.join("");
  }
}

// src/utils/sha256.ts
async function sha256(buffer) {
  if (buffer.size < 1e7 && _optionalChain([globalThis, 'access', _9 => _9.crypto, 'optionalAccess', _10 => _10.subtle])) {
    return hexFromBytes(
      new Uint8Array(
        await globalThis.crypto.subtle.digest("SHA-256", buffer instanceof Blob ? await buffer.arrayBuffer() : buffer)
      )
    );
  }
  if (isFrontend) {
    if (!wasmModule) {
      wasmModule = await Promise.resolve().then(() => _interopRequireWildcard(require("hash-wasm")));
    }
    const sha2562 = await wasmModule.createSHA256();
    sha2562.init();
    const reader = buffer.stream().getReader();
    while (true) {
      const { done, value } = await reader.read();
      if (done) {
        break;
      }
      sha2562.update(value);
    }
    return sha2562.digest("hex");
  }
  if (!cryptoModule) {
    cryptoModule = await Promise.resolve().then(() => _interopRequireWildcard(require("./sha256-node-C3QRUSZ5.js")));
  }
  return cryptoModule.sha256Node(buffer);
}
var cryptoModule;
var wasmModule;

// src/utils/toRepoId.ts
function toRepoId(repo) {
  if (typeof repo !== "string") {
    return repo;
  }
  if (repo.startsWith("model/") || repo.startsWith("models/")) {
    throw new TypeError(
      "A repo designation for a model should not start with 'models/', directly specify the model namespace / name"
    );
  }
  if (repo.startsWith("space/")) {
    throw new TypeError("Spaces should start with 'spaces/', plural, not 'space/'");
  }
  if (repo.startsWith("dataset/")) {
    throw new TypeError("Datasets should start with 'dataset/', plural, not 'dataset/'");
  }
  const slashes = repo.split("/").length - 1;
  if (repo.startsWith("spaces/")) {
    if (slashes !== 2) {
      throw new TypeError("Space Id must include namespace and name of the space");
    }
    return {
      type: "space",
      name: repo.slice("spaces/".length)
    };
  }
  if (repo.startsWith("datasets/")) {
    if (slashes > 2) {
      throw new TypeError("Too many slashes in repo designation: " + repo);
    }
    return {
      type: "dataset",
      name: repo.slice("datasets/".length)
    };
  }
  if (slashes > 1) {
    throw new TypeError("Too many slashes in repo designation: " + repo);
  }
  return {
    type: "model",
    name: repo
  };
}

// src/utils/WebBlob.ts
var WebBlob = class extends Blob {
  static async create(url, opts = { cacheBelow: 1e6 }) {
    const response = await fetch(url, { method: "HEAD" });
    const size = Number(response.headers.get("content-length"));
    const contentType = response.headers.get("content-type") || "";
    const supportRange = response.headers.get("accept-ranges") === "bytes";
    if (!supportRange || size < opts.cacheBelow) {
      return await (await fetch(url)).blob();
    }
    return new WebBlob(url, 0, size, contentType, true);
  }
  
  
  
  
  
  constructor(url, start, end, contentType, full) {
    super([]);
    this.url = url;
    this.start = start;
    this.end = end;
    this.contentType = contentType;
    this.full = full;
  }
  get size() {
    return this.end - this.start;
  }
  get type() {
    return this.contentType;
  }
  slice(start = 0, end = this.size) {
    if (start < 0 || end < 0) {
      new TypeError("Unsupported negative start/end on FileBlob.slice");
    }
    const slice = new WebBlob(
      this.url,
      this.start + start,
      Math.min(this.start + end, this.end),
      this.contentType,
      start === 0 && end === this.size ? this.full : false
    );
    return slice;
  }
  async arrayBuffer() {
    const result = await this.fetchRange();
    return result.arrayBuffer();
  }
  async text() {
    const result = await this.fetchRange();
    return result.text();
  }
  stream() {
    const stream = new TransformStream();
    this.fetchRange().then((response) => _optionalChain([response, 'access', _11 => _11.body, 'optionalAccess', _12 => _12.pipeThrough, 'call', _13 => _13(stream)])).catch((error) => stream.writable.abort(error.message));
    return stream.readable;
  }
  fetchRange() {
    if (this.full) {
      return fetch(this.url);
    }
    return fetch(this.url, {
      headers: {
        Range: `bytes=${this.start}-${this.end - 1}`
      }
    });
  }
};

// src/lib/commit.ts
var CONCURRENT_SHAS = 5;
var CONCURRENT_LFS_UPLOADS = 5;
var MULTIPART_PARALLEL_UPLOAD = 5;
function isFileOperation(op) {
  const ret = op.operation === "addOrUpdate";
  if (ret && !(op.content instanceof Blob)) {
    throw new TypeError("Precondition failed: op.content should be a Blob");
  }
  return ret;
}
async function createBlob(url) {
  if (url.protocol === "http:" || url.protocol === "https:") {
    return WebBlob.create(url);
  }
  if (isFrontend) {
    throw new TypeError(`Unsupported URL protocol "${url.protocol}"`);
  }
  if (url.protocol === "file:") {
    const { FileBlob } = await Promise.resolve().then(() => _interopRequireWildcard(require("./FileBlob-LROWI4DY.js")));
    return FileBlob.create(url);
  }
  throw new TypeError(`Unsupported URL protocol "${url.protocol}"`);
}
async function* commitIter(params) {
  checkCredentials(params.credentials);
  const repoId = toRepoId(params.repo);
  yield "preuploading";
  const lfsShas = /* @__PURE__ */ new Map();
  const allOperations = await Promise.all(
    params.operations.map(async (operation) => {
      if (operation.operation !== "addOrUpdate") {
        return operation;
      }
      if (!(operation.content instanceof URL)) {
        return { ...operation, content: operation.content };
      }
      const lazyBlob = await createBlob(operation.content);
      return {
        ...operation,
        content: lazyBlob
      };
    })
  );
  const gitAttributes = _optionalChain([allOperations, 'access', _14 => _14.filter, 'call', _15 => _15(isFileOperation), 'access', _16 => _16.find, 'call', _17 => _17((op) => op.path === ".gitattributes"), 'optionalAccess', _18 => _18.content]);
  for (const operations of chunk(allOperations.filter(isFileOperation), 100)) {
    const payload = {
      gitAttributes: gitAttributes && await gitAttributes.text(),
      files: await Promise.all(
        operations.map(async (operation) => ({
          path: operation.path,
          size: operation.content.size,
          sample: base64FromBytes(new Uint8Array(await operation.content.slice(0, 512).arrayBuffer()))
        }))
      )
    };
    const res2 = await fetch(
      `${_nullishCoalesce(params.hubUrl, () => ( HUB_URL))}/api/${repoId.type}s/${repoId.name}/preupload/${encodeURIComponent(
        _nullishCoalesce(params.branch, () => ( "main"))
      )}` + (params.isPullRequest ? "?create_pr=1" : ""),
      {
        method: "POST",
        headers: {
          Authorization: `Bearer ${params.credentials.accessToken}`,
          "Content-Type": "application/json"
        },
        body: JSON.stringify(payload)
      }
    );
    if (!res2.ok) {
      throw await createApiError(res2);
    }
    const json2 = await res2.json();
    for (const file of json2.files) {
      if (file.uploadMode === "lfs") {
        lfsShas.set(file.path, null);
      }
    }
  }
  yield "uploading to LFS";
  for (const operations of chunk(
    allOperations.filter(isFileOperation).filter((op) => lfsShas.has(op.path)),
    100
  )) {
    yield `hashing ${operations.length} files`;
    const shas = await promisesQueue(
      operations.map((op) => async () => {
        const sha = await sha256(op.content);
        lfsShas.set(op.path, sha);
        return sha;
      }),
      CONCURRENT_SHAS
    );
    const payload = {
      operation: "upload",
      // multipart is a custom protocol for HF
      transfers: ["basic", "multipart"],
      hash_algo: "sha_256",
      ref: {
        name: _nullishCoalesce(params.branch, () => ( "main"))
      },
      objects: operations.map((op, i) => ({
        oid: shas[i],
        size: op.content.size
      }))
    };
    const res2 = await fetch(
      `${_nullishCoalesce(params.hubUrl, () => ( HUB_URL))}/${repoId.type === "model" ? "" : repoId.type + "s/"}${repoId.name}.git/info/lfs/objects/batch`,
      {
        method: "POST",
        headers: {
          Authorization: `Bearer ${params.credentials.accessToken}`,
          Accept: "application/vnd.git-lfs+json",
          "Content-Type": "application/vnd.git-lfs+json"
        },
        body: JSON.stringify(payload)
      }
    );
    if (!res2.ok) {
      throw await createApiError(res2);
    }
    const json2 = await res2.json();
    const batchRequestId = res2.headers.get("X-Request-Id") || void 0;
    const shaToOperation = new Map(operations.map((op, i) => [shas[i], op]));
    await promisesQueueStreaming(
      json2.objects.map((obj) => async () => {
        const op = shaToOperation.get(obj.oid);
        if (!op) {
          throw new InvalidApiResponseFormatError("Unrequested object ID in response");
        }
        if (obj.error) {
          const errorMessage = `Error while doing LFS batch call for ${operations[shas.indexOf(obj.oid)].path}: ${obj.error.message}${batchRequestId ? ` - Request ID: ${batchRequestId}` : ""}`;
          throw new HubApiError(res2.url, obj.error.code, batchRequestId, errorMessage);
        }
        if (!_optionalChain([obj, 'access', _19 => _19.actions, 'optionalAccess', _20 => _20.upload])) {
          return;
        }
        const content = op.content;
        const header = obj.actions.upload.header;
        if (_optionalChain([header, 'optionalAccess', _21 => _21.chunk_size])) {
          const chunkSize = parseInt(header.chunk_size);
          const completionUrl = obj.actions.upload.href;
          const parts = Object.keys(header).filter((key) => /^[0-9]+$/.test(key));
          if (parts.length !== Math.ceil(content.size / chunkSize)) {
            throw new Error("Invalid server response to upload large LFS file, wrong number of parts");
          }
          const completeReq = {
            oid: obj.oid,
            parts: parts.map((part) => ({
              partNumber: +part,
              etag: ""
            }))
          };
          await promisesQueueStreaming(
            parts.map((part) => async () => {
              const index = parseInt(part) - 1;
              const slice = content.slice(index * chunkSize, (index + 1) * chunkSize);
              const res4 = await fetch(header[part], {
                method: "PUT",
                /** Unfortunately, browsers don't support our inherited version of Blob in fetch calls */
                body: slice instanceof WebBlob && isFrontend ? await slice.arrayBuffer() : slice
              });
              if (!res4.ok) {
                throw await createApiError(res4, {
                  requestId: batchRequestId,
                  message: `Error while uploading part ${part} of ${operations[shas.indexOf(obj.oid)].path} to LFS storage`
                });
              }
              const eTag = res4.headers.get("ETag");
              if (!eTag) {
                throw new Error("Cannot get ETag of part during multipart upload");
              }
              completeReq.parts[Number(part) - 1].etag = eTag;
            }),
            MULTIPART_PARALLEL_UPLOAD
          );
          const res3 = await fetch(completionUrl, {
            method: "POST",
            body: JSON.stringify(completeReq),
            headers: {
              Accept: "application/vnd.git-lfs+json",
              "Content-Type": "application/vnd.git-lfs+json"
            }
          });
          if (!res3.ok) {
            throw await createApiError(res3, {
              requestId: batchRequestId,
              message: `Error completing multipart upload of ${operations[shas.indexOf(obj.oid)].path} to LFS storage`
            });
          }
        } else {
          const res3 = await fetch(obj.actions.upload.href, {
            method: "PUT",
            headers: {
              ...batchRequestId ? { "X-Request-Id": batchRequestId } : void 0
            },
            /** Unfortunately, browsers don't support our inherited version of Blob in fetch calls */
            body: content instanceof WebBlob && isFrontend ? await content.arrayBuffer() : content
          });
          if (!res3.ok) {
            throw await createApiError(res3, {
              requestId: batchRequestId,
              message: `Error while uploading ${operations[shas.indexOf(obj.oid)].path} to LFS storage`
            });
          }
        }
      }),
      CONCURRENT_LFS_UPLOADS
    );
  }
  yield "committing";
  const res = await fetch(
    `${_nullishCoalesce(params.hubUrl, () => ( HUB_URL))}/api/${repoId.type}s/${repoId.name}/commit/${encodeURIComponent(
      _nullishCoalesce(params.branch, () => ( "main"))
    )}` + (params.isPullRequest ? "?create_pr=1" : ""),
    {
      method: "POST",
      headers: {
        Authorization: `Bearer ${params.credentials.accessToken}`,
        "Content-Type": "application/x-ndjson"
      },
      body: [
        {
          key: "header",
          value: {
            summary: params.title,
            description: params.description,
            parentCommit: params.parentCommit
          }
        },
        ...await Promise.all(
          allOperations.map((operation) => {
            if (isFileOperation(operation)) {
              const sha = lfsShas.get(operation.path);
              if (sha) {
                return {
                  key: "lfsFile",
                  value: {
                    path: operation.path,
                    algo: "sha256",
                    size: operation.content.size,
                    oid: sha
                  }
                };
              }
            }
            return convertOperationToNdJson(operation);
          })
        )
      ].map((x) => JSON.stringify(x)).join("\n")
    }
  );
  if (!res.ok) {
    throw await createApiError(res);
  }
  const json = await res.json();
  return {
    pullRequestUrl: json.pullRequestUrl,
    commit: {
      oid: json.commitOid,
      url: json.commitUrl
    },
    hookOutput: json.hookOutput
  };
}
async function commit(params) {
  const iterator = commitIter(params);
  let res = await iterator.next();
  while (!res.done) {
    res = await iterator.next();
  }
  return res.value;
}
async function convertOperationToNdJson(operation) {
  switch (operation.operation) {
    case "addOrUpdate": {
      return {
        key: "file",
        value: {
          content: base64FromBytes(new Uint8Array(await operation.content.arrayBuffer())),
          path: operation.path,
          encoding: "base64"
        }
      };
    }
    case "delete": {
      return {
        key: "deletedFile",
        value: {
          path: operation.path
        }
      };
    }
    default:
      throw new TypeError("Unknown operation: " + operation.operation);
  }
}

// src/lib/create-repo.ts
async function createRepo(params) {
  checkCredentials(params.credentials);
  const repoId = toRepoId(params.repo);
  const [namespace, repoName] = repoId.name.split("/");
  if (!namespace || !repoName) {
    throw new TypeError(
      `"${repoId.name}" is not a fully qualified repo name. It should be of the form "{namespace}/{repoName}".`
    );
  }
  const res = await fetch(`${_nullishCoalesce(params.hubUrl, () => ( HUB_URL))}/api/repos/create`, {
    method: "POST",
    body: JSON.stringify({
      name: repoName,
      private: params.private,
      organization: namespace,
      license: params.license,
      ...repoId.type === "space" ? {
        type: "space",
        sdk: "static"
      } : {
        type: repoId.type
      },
      files: params.files ? await Promise.all(
        params.files.map(async (file) => ({
          encoding: "base64",
          path: file.path,
          content: base64FromBytes(
            new Uint8Array(file.content instanceof Blob ? await file.content.arrayBuffer() : file.content)
          )
        }))
      ) : void 0
    }),
    headers: {
      Authorization: `Bearer ${params.credentials.accessToken}`,
      "Content-Type": "application/json"
    }
  });
  if (!res.ok) {
    throw await createApiError(res);
  }
  const output = await res.json();
  return { repoUrl: output.url };
}

// src/lib/delete-repo.ts
async function deleteRepo(params) {
  checkCredentials(params.credentials);
  const repoId = toRepoId(params.repo);
  const [namespace, repoName] = repoId.name.split("/");
  const res = await fetch(`${_nullishCoalesce(params.hubUrl, () => ( HUB_URL))}/api/repos/delete`, {
    method: "DELETE",
    body: JSON.stringify({
      name: repoName,
      organization: namespace,
      type: repoId.type
    }),
    headers: {
      Authorization: `Bearer ${params.credentials.accessToken}`,
      "Content-Type": "application/json"
    }
  });
  if (!res.ok) {
    throw await createApiError(res);
  }
}

// src/lib/delete-file.ts
function deleteFile(params) {
  return commit({
    credentials: params.credentials,
    repo: params.repo,
    operations: [
      {
        operation: "delete",
        path: params.path
      }
    ],
    title: _nullishCoalesce(params.commitTitle, () => ( `Delete ${params.path}`)),
    description: params.commitDescription,
    hubUrl: params.hubUrl,
    branch: params.branch,
    isPullRequest: params.isPullRequest,
    parentCommit: params.parentCommit
  });
}

// src/lib/delete-files.ts
function deleteFiles(params) {
  return commit({
    credentials: params.credentials,
    repo: params.repo,
    operations: params.paths.map((path) => ({
      operation: "delete",
      path
    })),
    title: _nullishCoalesce(params.commitTitle, () => ( `Deletes ${params.paths.length} files`)),
    description: params.commitDescription,
    hubUrl: params.hubUrl,
    branch: params.branch,
    isPullRequest: params.isPullRequest,
    parentCommit: params.parentCommit
  });
}

// src/lib/download-file.ts
async function downloadFile(params) {
  checkCredentials(params.credentials);
  const repoId = toRepoId(params.repo);
  const url = `${_nullishCoalesce(params.hubUrl, () => ( HUB_URL))}/${repoId.type === "model" ? "" : `${repoId.type}s/`}${repoId.name}/${params.raw ? "raw" : "resolve"}/${encodeURIComponent(_nullishCoalesce(params.revision, () => ( "main")))}/${params.path}`;
  const resp = await fetch(url, {
    headers: params.credentials ? {
      Authorization: `Bearer ${params.credentials.accessToken}`
    } : {}
  });
  if (resp.status === 404 && resp.headers.get("X-Error-Code") === "EntryNotFound") {
    return null;
  } else if (!resp.ok) {
    throw await createApiError(resp);
  }
  return resp;
}

// src/lib/file-download-info.ts
async function fileDownloadInfo(params) {
  checkCredentials(params.credentials);
  const repoId = toRepoId(params.repo);
  const hubUrl = _nullishCoalesce(params.hubUrl, () => ( HUB_URL));
  const url = `${hubUrl}/${repoId.type === "model" ? "" : `${repoId.type}s/`}${repoId.name}/${params.raw ? "raw" : "resolve"}/${encodeURIComponent(_nullishCoalesce(params.revision, () => ( "main")))}/${params.path}` + (params.noContentDisposition ? "?noContentDisposition=1" : "");
  const resp = await fetch(url, {
    method: "HEAD",
    headers: params.credentials ? {
      Authorization: `Bearer ${params.credentials.accessToken}`
    } : {}
  });
  if (resp.status === 404 && resp.headers.get("X-Error-Code") === "EntryNotFound") {
    return null;
  }
  if (!resp.ok) {
    throw await createApiError(resp);
  }
  const etag = resp.headers.get("ETag");
  if (!etag) {
    throw new InvalidApiResponseFormatError("Expected ETag");
  }
  const sizeHeader = resp.headers.get("Content-Length");
  if (!sizeHeader) {
    throw new InvalidApiResponseFormatError("Expected size information");
  }
  const size = parseInt(sizeHeader);
  if (isNaN(size)) {
    throw new InvalidApiResponseFormatError("Invalid file size received");
  }
  return {
    etag,
    size,
    downloadLink: new URL(resp.url).hostname !== new URL(hubUrl).hostname ? resp.url : null
  };
}

// src/utils/parseLinkHeader.ts
function parseLinkHeader(header) {
  const regex = /<(https?:[/][/][^>]+)>;\s+rel="([^"]+)"/g;
  return Object.fromEntries([...header.matchAll(regex)].map(([, url, rel]) => [rel, url]));
}

// src/lib/list-datasets.ts
var EXPAND_KEYS = ["private", "downloads", "gated", "likes", "lastModified"];
async function* listDatasets(params) {
  checkCredentials(_optionalChain([params, 'optionalAccess', _22 => _22.credentials]));
  const search = new URLSearchParams([
    ...Object.entries({
      limit: "500",
      ..._optionalChain([params, 'optionalAccess', _23 => _23.search, 'optionalAccess', _24 => _24.owner]) ? { author: params.search.owner } : void 0
    }),
    ...EXPAND_KEYS.map((val) => ["expand", val])
  ]).toString();
  let url = `${_optionalChain([params, 'optionalAccess', _25 => _25.hubUrl]) || HUB_URL}/api/datasets` + (search ? "?" + search : "");
  while (url) {
    const res = await fetch(url, {
      headers: {
        accept: "application/json",
        ..._optionalChain([params, 'optionalAccess', _26 => _26.credentials]) ? { Authorization: `Bearer ${params.credentials.accessToken}` } : void 0
      }
    });
    if (!res.ok) {
      throw createApiError(res);
    }
    const items = await res.json();
    for (const item of items) {
      yield {
        id: item._id,
        name: item.id,
        private: item.private,
        downloads: item.downloads,
        likes: item.likes,
        gated: item.gated,
        updatedAt: new Date(item.lastModified)
      };
    }
    const linkHeader = res.headers.get("Link");
    url = linkHeader ? parseLinkHeader(linkHeader).next : void 0;
  }
}

// src/lib/list-models.ts
var EXPAND_KEYS2 = ["pipeline_tag", "private", "gated", "downloads", "likes"];
async function* listModels(params) {
  checkCredentials(_optionalChain([params, 'optionalAccess', _27 => _27.credentials]));
  const search = new URLSearchParams([
    ...Object.entries({
      limit: "500",
      ..._optionalChain([params, 'optionalAccess', _28 => _28.search, 'optionalAccess', _29 => _29.owner]) ? { author: params.search.owner } : void 0,
      ..._optionalChain([params, 'optionalAccess', _30 => _30.search, 'optionalAccess', _31 => _31.task]) ? { pipeline_tag: params.search.task } : void 0
    }),
    ...EXPAND_KEYS2.map((val) => ["expand", val])
  ]).toString();
  let url = `${_optionalChain([params, 'optionalAccess', _32 => _32.hubUrl]) || HUB_URL}/api/models?${search}`;
  while (url) {
    const res = await fetch(url, {
      headers: {
        accept: "application/json",
        ..._optionalChain([params, 'optionalAccess', _33 => _33.credentials]) ? { Authorization: `Bearer ${params.credentials.accessToken}` } : void 0
      }
    });
    if (!res.ok) {
      throw createApiError(res);
    }
    const items = await res.json();
    for (const item of items) {
      yield {
        id: item._id,
        name: item.id,
        private: item.private,
        task: item.pipeline_tag,
        downloads: item.downloads,
        gated: item.gated,
        likes: item.likes,
        updatedAt: new Date(item.lastModified)
      };
    }
    const linkHeader = res.headers.get("Link");
    url = linkHeader ? parseLinkHeader(linkHeader).next : void 0;
  }
}

// src/lib/list-spaces.ts
var EXPAND_KEYS3 = ["sdk", "likes", "private", "lastModified"];
async function* listSpaces(params) {
  checkCredentials(_optionalChain([params, 'optionalAccess', _34 => _34.credentials]));
  const search = new URLSearchParams([
    ...Object.entries({ limit: "500", ..._optionalChain([params, 'optionalAccess', _35 => _35.search, 'optionalAccess', _36 => _36.owner]) ? { author: params.search.owner } : void 0 }),
    ...EXPAND_KEYS3.map((val) => ["expand", val])
  ]).toString();
  let url = `${_optionalChain([params, 'optionalAccess', _37 => _37.hubUrl]) || HUB_URL}/api/spaces?${search}`;
  while (url) {
    const res = await fetch(url, {
      headers: {
        accept: "application/json",
        ..._optionalChain([params, 'optionalAccess', _38 => _38.credentials]) ? { Authorization: `Bearer ${params.credentials.accessToken}` } : void 0
      }
    });
    if (!res.ok) {
      throw createApiError(res);
    }
    const items = await res.json();
    for (const item of items) {
      yield {
        id: item._id,
        name: item.id,
        sdk: item.sdk,
        likes: item.likes,
        private: item.private,
        updatedAt: new Date(item.lastModified)
      };
    }
    const linkHeader = res.headers.get("Link");
    url = linkHeader ? parseLinkHeader(linkHeader).next : void 0;
  }
}

// src/lib/list-files.ts
async function* listFiles(params) {
  checkCredentials(params.credentials);
  const repoId = toRepoId(params.repo);
  let url = `${params.hubUrl || HUB_URL}/api/${repoId.type}s/${repoId.name}/tree/${params.revision || "main"}${params.path ? "/" + params.path : ""}?recursive=${!!params.recursive}&expand=${!!params.expand}`;
  while (url) {
    const res = await fetch(url, {
      headers: {
        accept: "application/json",
        ...params.credentials ? { Authorization: `Bearer ${params.credentials.accessToken}` } : void 0
      }
    });
    if (!res.ok) {
      throw createApiError(res);
    }
    const items = await res.json();
    for (const item of items) {
      yield item;
    }
    const linkHeader = res.headers.get("Link");
    url = linkHeader ? parseLinkHeader(linkHeader).next : void 0;
  }
}

// src/lib/upload-file.ts
function uploadFile(params) {
  const path = params.file instanceof URL ? _nullishCoalesce(params.file.pathname.split("/").at(-1), () => ( "file")) : "path" in params.file ? params.file.path : params.file.name;
  return commit({
    credentials: params.credentials,
    repo: params.repo,
    operations: [
      {
        operation: "addOrUpdate",
        path,
        content: "content" in params.file ? params.file.content : params.file
      }
    ],
    title: _nullishCoalesce(params.commitTitle, () => ( `Add ${path}`)),
    description: params.commitDescription,
    hubUrl: params.hubUrl,
    branch: params.branch,
    isPullRequest: params.isPullRequest,
    parentCommit: params.parentCommit
  });
}

// src/lib/upload-files.ts
function uploadFiles(params) {
  return commit({
    credentials: params.credentials,
    repo: params.repo,
    operations: params.files.map((file) => ({
      operation: "addOrUpdate",
      path: file instanceof URL ? _nullishCoalesce(file.pathname.split("/").at(-1), () => ( "file")) : "path" in file ? file.path : file.name,
      content: "content" in file ? file.content : file
    })),
    title: _nullishCoalesce(params.commitTitle, () => ( `Add ${params.files.length} files`)),
    description: params.commitDescription,
    hubUrl: params.hubUrl,
    branch: params.branch,
    isPullRequest: params.isPullRequest,
    parentCommit: params.parentCommit
  });
}

// src/lib/who-am-i.ts
async function whoAmI(params) {
  checkCredentials(params.credentials);
  const res = await fetch(`${_nullishCoalesce(params.hubUrl, () => ( HUB_URL))}/api/whoami-v2`, {
    headers: {
      Authorization: `Bearer ${params.credentials.accessToken}`
    }
  });
  if (!res.ok) {
    throw await createApiError(res);
  }
  const response = await res.json();
  if (typeof _optionalChain([response, 'access', _39 => _39.auth, 'access', _40 => _40.accessToken, 'optionalAccess', _41 => _41.expiration]) === "string") {
    response.auth.accessToken.expiration = new Date(response.auth.accessToken.expiration);
  }
  return response;
}

















exports.HubApiError = HubApiError; exports.InvalidApiResponseFormatError = InvalidApiResponseFormatError; exports.commit = commit; exports.createRepo = createRepo; exports.deleteFile = deleteFile; exports.deleteFiles = deleteFiles; exports.deleteRepo = deleteRepo; exports.downloadFile = downloadFile; exports.fileDownloadInfo = fileDownloadInfo; exports.listDatasets = listDatasets; exports.listFiles = listFiles; exports.listModels = listModels; exports.listSpaces = listSpaces; exports.uploadFile = uploadFile; exports.uploadFiles = uploadFiles; exports.whoAmI = whoAmI;
